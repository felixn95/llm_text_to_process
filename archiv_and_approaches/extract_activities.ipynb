{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# read the excel file\n",
    "df = pd.read_excel('data/data_relations.xlsx')\n",
    "\n",
    "# filter the rows where the \"document-name\" column is equal to \"doc-10.1\"\n",
    "# df_doc10 = df[df['document name'] == 'doc-10.1']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the prompt template\n",
    "template = \"\"\"Great, now do the same for this:\n",
    "\n",
    "In the following there will be business process model description in tokens. \n",
    "\n",
    "In the same sequence of the tokens, ther are also the annotations for the tokens, here is an explanation about the tags: \n",
    "O: An O tag indicates that a token belongs to no chunk.\n",
    "B-Actor: This tag indicates the beginning of an Actor chunk.\n",
    "I-Actor: This tag indicates that the tag is inside an Actor chunk.\n",
    "B-Activity: This tag indicates the beginning of an Activity chunk.\n",
    "I-Activity: This tag indicates that the tag is inside an Activity chunk.\n",
    "B-Activity Data: This tag indicates the beginning of an Activity Data chunk.\n",
    "I-Activity Data: This tag indicates that the tag is inside an Activity Data chunk.\n",
    "B-Further Specification: This tag indicates the beginning of a Further Specification chunk.\n",
    "I-Further Specification: This tag indicates that the tag is inside a Further Specification chunk.\n",
    "B-XOR Gateway: This tag indicates the beginning of a XOR Gateway chunk.\n",
    "I-XOR Gateway: This tag indicates that the tag is inside a XOR Gateway chunk.\n",
    "B-Condition Specification: This tag indicates the beginning of a Condition Specification chunk.\n",
    "I-Condition Specification: This tag indicates that the tag is inside a Condition Specification chunk.\n",
    "B-AND Gateway: This tag indicates the beginning of an AND Gateway chunk.\n",
    "I-AND Gateway: This tag indicates that the tag is inside an AND Gateway chunk.\n",
    "\n",
    "tokens:\n",
    "{tokens}\n",
    "tags:\n",
    "{ner_tags}\n",
    "\n",
    "Please extract all Activities and all actors out of this process for me. Please extract them as arrays. There is no need to write code or something, just give me the extracted arrays.\"\"\"\n",
    "\n",
    "# Create a new DataFrame for the output\n",
    "output_df = pd.DataFrame()\n",
    "output_df[\"document name\"] = df[\"document name\"]\n",
    "\n",
    "# Populate the \"prompts\" column\n",
    "output_df[\"prompts\"] = df.apply(lambda row: template.format(tokens=row[\"tokens\"], ner_tags=row[\"ner_tags\"]), axis=1)\n",
    "\n",
    "# Save the new DataFrame to an Excel file\n",
    "output_df.to_excel(\"output_file_prompts.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document name,tokens,tokens-IDs,ner_tags,sentence-IDs,relations\n",
      "doc-10.1,\"['The', 'MPON', 'sents', 'the', 'dismissal', 'to', 'the', 'MPOO', '.', 'The', 'MPOO', 'reviews', 'the', 'dismissal', '.', 'The', 'MPOO', 'opposes', 'the', 'dismissal', 'of', 'MPON', 'or', 'the', 'MPOO', 'confirmes', 'the', 'dismissal', 'of', 'the', 'MPON', '.']\",\"[0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\",\"['B-Actor', 'I-Actor', 'B-Activity', 'B-Activity Data', 'I-Activity Data', 'O', 'B-Actor', 'I-Actor', 'O', 'B-Actor', 'I-Actor', 'B-Activity', 'B-Activity Data', 'I-Activity Data', 'O', 'B-Actor', 'I-Actor', 'B-Activity', 'B-Activity Data', 'I-Activity Data', 'O', 'O', 'B-XOR Gateway', 'B-Actor', 'I-Actor', 'B-Activity', 'B-Activity Data', 'I-Activity Data', 'O', 'O', 'O', 'O']\",\"[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\",\"{'source-head-sentence-ID': [0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 2, 2], 'source-head-word-ID': [2, 2, 2, 2, 2, 2, 2, 2, 2, 7, 7, 10, 10], 'relation-type': ['uses', 'actor recipient', 'actor performer', 'flow', 'uses', 'actor performer', 'flow', 'uses', 'actor performer', 'flow', 'flow', 'uses', 'actor performer'], 'target-head-sentence-ID': [0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2], 'target-head-word-ID': [3, 6, 0, 2, 3, 0, 7, 3, 0, 2, 10, 11, 8]}\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df_doc10.head(1).to_csv(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast \n",
    "\n",
    "# Function to extract untokenized sentences from the tokens\n",
    "def extract_sentences(tokens):\n",
    "    if isinstance(tokens, str):\n",
    "        import ast\n",
    "        tokens = ast.literal_eval(tokens)\n",
    "\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token != '.':\n",
    "            sentence.append(token)\n",
    "        else:\n",
    "            sentence.append(token)\n",
    "            sentences.append(' '.join(sentence))\n",
    "            sentence = []\n",
    "            \n",
    "    return sentences\n",
    "\n",
    "# Read the original excel file\n",
    "df = pd.read_excel('data/data_relations.xlsx')\n",
    "\n",
    "# Extract sentences for each row and transform them into single strings\n",
    "all_texts = df['tokens'].apply(lambda x: ' '.join(extract_sentences(x)))\n",
    "\n",
    "# Create a new DataFrame with the desired format\n",
    "new_df = pd.DataFrame({\n",
    "    'document name': df['document name'],\n",
    "    'process_text_raw': all_texts\n",
    "})\n",
    "\n",
    "# Save the new DataFrame to an Excel file\n",
    "new_df.to_excel('processed_data_with_names.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The largest entry has 769 tokens.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "max_tokens = df['tokens'].apply(lambda x: len(ast.literal_eval(x))).max()\n",
    "print(f\"The largest entry has {max_tokens} tokens.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
